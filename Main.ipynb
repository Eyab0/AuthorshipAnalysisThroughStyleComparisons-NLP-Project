{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taamolat data set\n",
    "taamolat_directory  = 'Authors Dataset/taamolat/'\n",
    "TAAMOLAT_NUMBER_OF_AUTHORS = 7\n",
    "TAAMOLAT_NUMBER_OF_SAMPLES_PER_AUTHOR = 10\n",
    "\n",
    "# alaraby data set\n",
    "alaraby_directory = 'Authors Dataset/alaraby/'\n",
    "ALARABY_NUMBER_OF_AUTHORS = 9\n",
    "ALARABY_NUMBER_OF_SAMPLES_PER_AUTHOR = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for return the list of authors names for a specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_names(directory_name, number_of_authors):\n",
    "    import configparser\n",
    "\n",
    "    # Create a ConfigParser object\n",
    "    config = configparser.ConfigParser()\n",
    "\n",
    "    # save the authors names in a list\n",
    "    authors = []\n",
    "    for i in range(number_of_authors):\n",
    "        # Read the properties file with the correct encoding (e.g., 'utf-8')\n",
    "        with open(f'{directory_name}a{i}/a{i}_0.properties', encoding='utf-8') as f:\n",
    "            # Add a default section header to the file\n",
    "            content = '[default]\\n' + f.read()\n",
    "            config.read_string(content)\n",
    "\n",
    "        # Get the author name\n",
    "        # Get the value of the 'author_name' key\n",
    "        author_name = config.get('default', 'author_name')\n",
    "        authors.append(author_name)\n",
    "\n",
    "    return authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "taamolat_authors_english = get_authors_names(taamolat_directory, TAAMOLAT_NUMBER_OF_AUTHORS)\n",
    "# manually add the arabic names of the authors of the taamolat dataset\n",
    "taamolat_authors_names = ['محمد عواد', 'محمد زينوبة', 'يونس جعدي', 'نور الدين رافع', 'عبير قلوطي', 'معتز خطيب', 'إبراهيم نصر الله']\n",
    "taamolat_authors = [ i for i in range(TAAMOLAT_NUMBER_OF_AUTHORS) ]\n",
    "alaraby_authors_names = get_authors_names(alaraby_directory, ALARABY_NUMBER_OF_AUTHORS)\n",
    "alaraby_authors = [ i for i in range(TAAMOLAT_NUMBER_OF_AUTHORS, ALARABY_NUMBER_OF_AUTHORS + TAAMOLAT_NUMBER_OF_AUTHORS) ]\n",
    "\n",
    "# print(taamolat_authors[0], taamolat_authors_names[0])\n",
    "# print(alaraby_authors[0], alaraby_authors_names[0])\n",
    "\n",
    "# print(taamolat_authors)\n",
    "# print(alaraby_authors)\n",
    "\n",
    "all_authors_names = taamolat_authors_names + alaraby_authors_names\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the paragraphs of the authors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each paragraph with its author\n",
    "def get_paragraphs_with_authors(directory_name, number_of_authors, number_of_samples_per_author, authors_names):\n",
    "    paragraphs = []\n",
    "    for i in range(number_of_authors):\n",
    "        for j in range(number_of_samples_per_author):\n",
    "            with open(f'{directory_name}a{i}/sample{j}.txt', encoding='utf-8') as file:\n",
    "                current_paragraph = ''\n",
    "    \n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                \n",
    "                    if line:  # If the line is not empty\n",
    "                        current_paragraph += line + ' '\n",
    "                    elif current_paragraph:  # If an empty line is encountered and there is a current paragraph\n",
    "                        paragraphs.append((current_paragraph.strip(), authors_names[i]))\n",
    "                        current_paragraph = ''\n",
    "\n",
    "                if current_paragraph:  # Append the last paragraph if it exists after reaching the end of the file\n",
    "                    paragraphs.append((current_paragraph.strip(), authors_names[i]))\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "taamolat_data = get_paragraphs_with_authors(taamolat_directory, TAAMOLAT_NUMBER_OF_AUTHORS, TAAMOLAT_NUMBER_OF_SAMPLES_PER_AUTHOR, taamolat_authors)\n",
    "alaraby_data = get_paragraphs_with_authors(alaraby_directory, ALARABY_NUMBER_OF_AUTHORS, ALARABY_NUMBER_OF_SAMPLES_PER_AUTHOR, alaraby_authors)\n",
    "\n",
    "# for paragraph in taamolat_data:\n",
    "#     print(paragraph)\n",
    "#     print()\n",
    "\n",
    "# for paragraph in alaraby_data:\n",
    "#     print(paragraph)\n",
    "#     print()\n",
    "\n",
    "\n",
    "# combine the two datasets\n",
    "data_set = taamolat_data + alaraby_data\n",
    "# make sure that the data is shuffled\n",
    "import random\n",
    "random.shuffle(data_set)\n",
    "\n",
    "number_of_samples = len(data_set)\n",
    "\n",
    "\n",
    "# for paragraph in data_set:\n",
    "#     print(paragraph)\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing The data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يشار الا ههاغالاالان اللغه العربيه\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Normalization: remove diacritics from the text & Punctuation mark removal & Numbers & Non-arabic & alef, teh \n",
    "def normalization(text):\n",
    "    import re\n",
    "    import tnkeeh\n",
    "    from pyarabic.araby import strip_tashkeel, normalize_alef, normalize_teh\n",
    "    text = re.sub(r'[^\\u0621-\\u064A\\s]', '', text)\n",
    "    text = strip_tashkeel(text)\n",
    "    text = normalize_alef(text)\n",
    "    text = normalize_teh(text)\n",
    "    tnkeeh_obj = tnkeeh.Tnkeeh(\n",
    "    remove_special_chars=True,\n",
    "    remove_english=True, normalize=True, remove_diacritics=True,\n",
    "    remove_tatweel=True, remove_html_elements=True,\n",
    "    remove_links=True, remove_twitter_meta=True\n",
    "    )\n",
    "    text = tnkeeh_obj.clean_raw_text(text)[0]\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = \"يْشَارْ إِلَasfgى ةهأغإلأألأَنْ اللُّغَ15+62ة الْعَ?!#@$#^رْبِيَّة\"\n",
    "text = normalization(text)\n",
    "print(text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "درس\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stemming\n",
    "def stemming(word):\n",
    "    from nltk.stem.isri import ISRIStemmer # special stemmer for arabic\n",
    "    # Create an instance of the ISRI Arabic Stemmer\n",
    "\n",
    "    stemmer = ISRIStemmer()\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "text = 'المدرسة'\n",
    "result = stemming(text)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# POS tagging\n",
    "def pos_tagging(text):\n",
    "\n",
    "    # from farasa.pos import FarasaPOSTagger\n",
    "    # tagger = FarasaPOSTagger()\n",
    "    # tags = tagger.tag_segments(text, combine_subtokens=True)\n",
    "    # return tags\n",
    "\n",
    "    # import json\n",
    "    # import requests\n",
    "    # api_key = \"mgmfGsWLmIFJznbMaC\"\n",
    "\n",
    "    # url = 'https://farasa.qcri.org/webapi/pos/'\n",
    "    # payload = {'text': text, 'api_key': api_key}\n",
    "    # data = requests.post(url, data=payload)\n",
    "    # result = json.loads(data.text)\n",
    "\n",
    "    # result = result['text']\n",
    "\n",
    "    # words_pos_tags = []\n",
    "    # for obj in result:\n",
    "    #     words_pos_tags.append((obj['surface'], obj['POS']))\n",
    "\n",
    "    # # remove the first and last elements of the list\n",
    "    # words_pos_tags = words_pos_tags[1:-1]\n",
    "\n",
    "    # return words_pos_tags\n",
    "    pass\n",
    "\n",
    "\n",
    "text = 'يُشار إلى أن اللغة العربية'\n",
    "result = pos_tagging(text)\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['يُشار', 'إلى', 'أن', 'اللغة', 'العربية']\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "def tokenization(text):\n",
    "    from pyarabic.araby import tokenize\n",
    "    \n",
    "    text = tokenize(text)\n",
    "    return text\n",
    "\n",
    "text = 'يُشار إلى أن اللغة العربية'\n",
    "\n",
    "result = tokenization(text)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_file_name = 'stop_words.txt'\n",
    "stop_words = []\n",
    "with open(stop_words_file_name, encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        stop_words.append(line)\n",
    "\n",
    "stop_words = list(set(stop_words))\n",
    "stop_words += stopwords.words('arabic')\n",
    "\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words, stop_words=stop_words):\n",
    "    \n",
    "    result = [] \n",
    "    removed_indexes = []\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in stop_words:\n",
    "            removed_indexes.append(i)\n",
    "        else:\n",
    "            result.append(words[i])\n",
    "        \n",
    "    removed_indexes.sort(reverse=True) # for save remove of pos tagging \n",
    "    return result, removed_indexes\n",
    "\n",
    "# remove_stop_words(['هذا', 'هو', 'من', 'الكلمات', 'المستبعدة', 'من', 'التحليل'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: at the beginning we made normlization for the text, then make tokenization for remove the stop words and apply stemming on the words without stop words, \n",
    "after that find the pos tagging on the normalized text with stop words to make corrcot results, the remove the pos tags of the stop words, and just save the stemming words with thier pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def preprocessing(text):\n",
    "#     normalization_text = normalization(text)\n",
    "#     segments = tokenization(normalization_text)\n",
    "\n",
    "#     # remove stop words from the the text after normalization\n",
    "#     words, removed_indexes = remove_stop_words(segments)\n",
    "#     number_of_words = len(words)\n",
    "    \n",
    "#     # find the pos tags for the words in the original text after normalization\n",
    "#     tags = pos_tagging(normalization_text)\n",
    "   \n",
    "\n",
    "#     # remove the tags of the stop words\n",
    "#     for index in removed_indexes:\n",
    "#         tags.pop(index)\n",
    "\n",
    "#     # stemming for the words in the original text after tokenization without stop words\n",
    "#     for i in range(number_of_words):\n",
    "#         words[i] = stemming(words[i])\n",
    "\n",
    "#     sample = []\n",
    "\n",
    "#     # combine the stemming words with their pos tags\n",
    "#     for i in range(number_of_words):\n",
    "#         sample.append((words[i], tags[i][1]))\n",
    "\n",
    "    \n",
    "#     return sample\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    normalization_text = normalization(text)\n",
    "    segments = tokenization(normalization_text)\n",
    "\n",
    "    # remove stop words from the the text after tokenization\n",
    "    words, _ = remove_stop_words(segments)\n",
    "    number_of_words = len(words)\n",
    "    \n",
    "    \n",
    "    # stemming for the words in the original text after tokenization without stop words\n",
    "    for i in range(number_of_words):\n",
    "        words[i] = stemming(words[i])\n",
    "\n",
    " \n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# text = \"أهلاً بكم! هذه الجملة تحتاج للتنظيف. 123\"\n",
    "\n",
    "# result = preprocessing(text)\n",
    "# print(result)\n",
    "# for word in result:\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_of_samples):\n",
    "    data_set[i] = (preprocessing(data_set[i][0]), data_set[i][1]) # (words, author)\n",
    "\n",
    "# print(data_set)\n",
    "# for sample in data_set:\n",
    "#     print(sample)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TFIDFExtractor:\n",
    "    def __init__(self):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.feature_names = None\n",
    "        self.author_names = None\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        # Extract the sentences from the corpus\n",
    "        sentences = [sample[0] for sample in corpus]\n",
    "\n",
    "        # Extract the authors from the corpus\n",
    "        authors = [sample[1] for sample in corpus]\n",
    "\n",
    "        # Convert the sentences to strings\n",
    "        sentences = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "        # Fit and transform the sentences\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(sentences)\n",
    "\n",
    "        # Get the feature names (unique words) as columns\n",
    "        self.feature_names = self.vectorizer.get_feature_names()\n",
    "\n",
    "        # Get the author names as rows\n",
    "        self.author_names = authors\n",
    "\n",
    "        return tfidf_matrix.toarray()\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        # Extract the sentences from the corpus\n",
    "        sentences = [sample[0] for sample in corpus]\n",
    "\n",
    "        # Convert the sentences to strings\n",
    "        sentences = [' '.join(sentence) for sentence in sentences]\n",
    "\n",
    "        # Transform the sentences\n",
    "        tfidf_matrix = self.vectorizer.transform(sentences)\n",
    "\n",
    "        return tfidf_matrix.toarray()\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def get_author_names(self):\n",
    "        return self.author_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tf_idf = TFIDFExtractor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WordEmbeddingExtractor:\n",
    "    def __init__(self, vector_size=100, window=10, min_count=1, workers=4, sg=0):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.sg = sg\n",
    "        self.model = None\n",
    "        self.feature_names = None\n",
    "        self.author_names = None\n",
    "\n",
    "    def fit_transform(self, corpus):\n",
    "        from gensim.models import FastText\n",
    "        import numpy as np\n",
    "        # Extract the sentences from the corpus\n",
    "        sentences = [sample[0] for sample in corpus]\n",
    "\n",
    "        # Extract the authors from the corpus\n",
    "        authors = [sample[1] for sample in corpus]\n",
    "\n",
    "        # Train FastText model\n",
    "        self.model = FastText(sentences, vector_size=self.vector_size, window=self.window,\n",
    "                              min_count=self.min_count, workers=self.workers, sg=self.sg)\n",
    "\n",
    "        # Get the feature names (labeled dimensions) for FastText\n",
    "        self.feature_names = [f't{i}' for i in range(self.vector_size)]\n",
    "\n",
    "        # Get the author names as rows\n",
    "        self.author_names = authors\n",
    "\n",
    "        # Create feature vectors for each sentence\n",
    "        feature_vectors = []\n",
    "        for sentence in sentences:\n",
    "            vectors = [self.model.wv[word] for word in sentence if word in self.model.wv]\n",
    "            if vectors:\n",
    "                sentence_vector = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                sentence_vector = np.zeros(self.vector_size)\n",
    "            feature_vectors.append(sentence_vector)\n",
    "\n",
    "        # Convert feature vectors to NumPy array\n",
    "        feature_vectors = np.array(feature_vectors)\n",
    "\n",
    "        return feature_vectors\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        import numpy as np\n",
    "        # Extract the sentences from the corpus\n",
    "        sentences = [sample[0] for sample in corpus]\n",
    "\n",
    "        # Create feature vectors for each sentence\n",
    "        feature_vectors = []\n",
    "        for sentence in sentences:\n",
    "            vectors = [self.model.wv[word] for word in sentence if word in self.model.wv]\n",
    "            if vectors:\n",
    "                sentence_vector = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                sentence_vector = np.zeros(self.vector_size)\n",
    "            feature_vectors.append(sentence_vector)\n",
    "\n",
    "        # Convert feature vectors to NumPy array\n",
    "        feature_vectors = np.array(feature_vectors)\n",
    "\n",
    "        return feature_vectors\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self.feature_names\n",
    "    \n",
    "    def get_author_names(self):\n",
    "        return self.author_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fast_text = WordEmbeddingExtractor(vector_size=100, window=10, min_count=1, workers=4, sg=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine features vectors (TF-IDF + FastText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6276\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = tf_idf.fit_transform(data_set)\n",
    "tf_idf_feature_names, authors_list = tf_idf.get_feature_names(), tf_idf.get_author_names()\n",
    "word_embedding_matrix = fast_text.fit_transform(data_set)\n",
    "word_embedding_feature_names, authors_list = fast_text.get_feature_names(), fast_text.get_author_names()\n",
    "\n",
    "\n",
    "\n",
    "# Combine the feature names\n",
    "combined_feature_names = tf_idf_feature_names + word_embedding_feature_names\n",
    "\n",
    "def combine_feature_vectors(x, y):\n",
    "    import numpy as np\n",
    "    # Combine the feature vectors\n",
    "    return np.concatenate((x, y), axis=1)\n",
    "\n",
    "# Combine the TF-IDF matrix and FastText feature vectors\n",
    "combined_features_vectors = combine_feature_vectors(tf_idf_matrix, word_embedding_matrix)\n",
    "\n",
    "\n",
    "# Create a list of tuples for feature vector-author mapping\n",
    "features_vectors_matrix = list(zip(combined_features_vectors, authors_list))\n",
    "\n",
    "print(len(features_vectors_matrix[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_features_vectors, authors_list, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, X_test, y_test):\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "    # Predict the labels for the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the precision\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Calculate the recall\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return precision, recall, f1, accuracy\n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Precision: 0.8869\n",
      "SVM Recall: 0.7649\n",
      "SVM F1 score: 0.7993\n",
      "SVM Accuracy: 0.7931\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create and train the SVM model with the RBF kernel\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1, accuracy = evaluation(svm_model, X_test, y_test)\n",
    "\n",
    "print(f'SVM Precision: {precision:.4f}')\n",
    "print(f'SVM Recall: {recall:.4f}')\n",
    "print(f'SVM F1 score: {f1:.4f}')\n",
    "print(f'SVM Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Precision: 0.7839\n",
      "Logistic Regression Recall: 0.5731\n",
      "Logistic Regression F1 score: 0.5900\n",
      "Logistic Regression Accuracy: 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\swata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_logreg = logreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1, accuracy = evaluation(logreg_model, X_test, y_test)\n",
    "\n",
    "print(f'Logistic Regression Precision: {precision:.4f}')\n",
    "print(f'Logistic Regression Recall: {recall:.4f}')\n",
    "print(f'Logistic Regression F1 score: {f1:.4f}')\n",
    "print(f'Logistic Regression Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Precision: 0.6656\n",
      "Random Forest Recall: 0.4586\n",
      "Random Forest F1 score: 0.4610\n",
      "Random Forest Accuracy: 0.5329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swata\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create and train the random forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1, accuracy = evaluation(rf_model, X_test, y_test)\n",
    "\n",
    "print(f'Random Forest Precision: {precision:.4f}')\n",
    "print(f'Random Forest Recall: {recall:.4f}')\n",
    "print(f'Random Forest F1 score: {f1:.4f}')\n",
    "print(f'Random Forest Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Precision: 0.7834\n",
      "KNN Recall: 0.7266\n",
      "KNN F1 score: 0.7367\n",
      "KNN Accuracy: 0.7335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1, accuracy = evaluation(knn_model, X_test, y_test)\n",
    "\n",
    "print(f'KNN Precision: {precision:.4f}')\n",
    "print(f'KNN Recall: {recall:.4f}')\n",
    "print(f'KNN F1 score: {f1:.4f}')\n",
    "print(f'KNN Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Precision: 0.9001\n",
      "MLP Recall: 0.8197\n",
      "MLP F1 score: 0.8471\n",
      "MLP Accuracy: 0.8339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier  # FOR CREATE NEURAL NETWORK MODEL\n",
    "\n",
    "# Create and train the MLP model\n",
    "mlp_model = MLPClassifier()\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_mlp = mlp_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "precision, recall, f1, accuracy = evaluation(mlp_model, X_test, y_test)\n",
    "\n",
    "print(f'MLP Precision: {precision:.4f}')\n",
    "print(f'MLP Recall: {recall:.4f}')\n",
    "print(f'MLP F1 score: {f1:.4f}')\n",
    "print(f'MLP Accuracy: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(text, model):\n",
    "    processed_text = preprocessing(text)\n",
    "    tf_idf_vector = tf_idf.transform([(processed_text, 0)])\n",
    "    word_embedding_vector = fast_text.transform([(processed_text, 0)])\n",
    "    combined_features_vector = combine_feature_vectors(tf_idf_vector, word_embedding_vector)\n",
    "    return model.predict(combined_features_vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For check different texts from same authors or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb7e71b1a214c5fbcbabae2663d5446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text 1:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88332d0fc05e4b259ae601ef225b30a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(width='400px'), rows=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f149b464ea24278a6274dbf6409825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text 2:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73be706460b942878a3248ac309ab662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(width='400px'), rows=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb5b96e64004421a47dfadac5c51f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('SVM Model', 'KNN Model', 'Random Forest Model', 'Logistic Regression …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeed13f3f38419f8f9cf1ed1b82aeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f1a6b5fd6843c0a2cd77ecc10c425a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Result:', layout=Layout(width='1000px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the available models\n",
    "models = {\n",
    "    'SVM Model': svm_model,\n",
    "    'KNN Model': knn_model,\n",
    "    'Random Forest Model': rf_model,\n",
    "    'Logistic Regression Model': logreg_model,\n",
    "    'MLP Model': mlp_model\n",
    "}\n",
    "\n",
    "# Create the dropdown widget\n",
    "model_dropdown = widgets.Dropdown(options=models.keys(), description='Model:')\n",
    "\n",
    "def process_text(btn):\n",
    "    input_text1 = input_field1.value.strip()  # Get input text from the first text field\n",
    "    input_text2 = input_field2.value.strip()  # Get input text from the second text field\n",
    "\n",
    "    # Get the selected model\n",
    "    selected_model = models[model_dropdown.value]\n",
    "\n",
    "    text1_result = modeling(input_text1, selected_model)[0]  # Get the result for the first text\n",
    "    text2_result = modeling(input_text2, selected_model)[0] # Get the result for the second text\n",
    "\n",
    "\n",
    "    author1 = all_authors_names[text1_result]\n",
    "    author2 = all_authors_names[text2_result]\n",
    "\n",
    "    # Display the results\n",
    "    result_label.value = f'Result: {\"Same Author\" if author1 == author2 else \"Different Authors \"} Text 1 is written by {author1} and Text 2 is written by {author2}'\n",
    "\n",
    "\n",
    "# Create the labels\n",
    "label1 = widgets.Label(value='Text 1:')\n",
    "label2 = widgets.Label(value='Text 2:')\n",
    "result_label = widgets.Label(value='Result:', layout=widgets.Layout(width='1000px'))\n",
    "\n",
    "# Create the input fields\n",
    "input_field1 = widgets.Textarea(value='', rows=5, layout=widgets.Layout(width='400px'))\n",
    "input_field2 = widgets.Textarea(value='', rows=5, layout=widgets.Layout(width='400px'))\n",
    "\n",
    "# Create the button\n",
    "button = widgets.Button(description='Process')\n",
    "button.on_click(process_text)\n",
    "\n",
    "# Display the GUI elements\n",
    "display(label1, input_field1, label2, input_field2, model_dropdown, button, result_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8ca1b673af42e5940689a17df8376d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Text:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52735ad58b7d46949cea7c0ca0e005e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(width='400px'), rows=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bc1bb69651461d9c30b0be73f98a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('SVM Model', 'KNN Model', 'Random Forest Model', 'Logistic Regression …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6981059863442199c4c8a024674628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038b8fac47814960af379936a993135d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Result:', layout=Layout(width='1000px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the available models\n",
    "models = {\n",
    "    'SVM Model': svm_model,\n",
    "    'KNN Model': knn_model,\n",
    "    'Random Forest Model': rf_model,\n",
    "    'Logistic Regression Model': logreg_model,\n",
    "    'MLP Model': mlp_model\n",
    "}\n",
    "\n",
    "# Create the dropdown widget\n",
    "model_dropdown = widgets.Dropdown(options=models.keys(), description='Model:')\n",
    "\n",
    "def process_text(btn):\n",
    "    input_text1 = input_field1.value.strip()  # Get input text from the first text field\n",
    "\n",
    "     # Get the selected model\n",
    "    selected_model = models[model_dropdown.value]\n",
    "\n",
    "    text1_result = modeling(input_text1, selected_model)[0]  # Get the result for the first text\n",
    "\n",
    "\n",
    "    author1 = all_authors_names[text1_result]\n",
    "\n",
    "    # Display the results\n",
    "    result_label.value = f'Result: the Text is written by {author1}'\n",
    "\n",
    "\n",
    "# Create the labels\n",
    "label1 = widgets.Label(value='Text:')\n",
    "\n",
    "result_label = widgets.Label(value='Result:', layout=widgets.Layout(width='1000px'))\n",
    "\n",
    "# Create the input fields\n",
    "input_field1 = widgets.Textarea(value='', rows=5, layout=widgets.Layout(width='400px'))\n",
    "\n",
    "# Create the button\n",
    "button = widgets.Button(description='Process')\n",
    "button.on_click(process_text)\n",
    "\n",
    "# Display the GUI elements\n",
    "display(label1, input_field1, model_dropdown, button, result_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
